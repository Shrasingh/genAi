 1. What is Masked Self-Attention?
Masked Self-Attention is a variant of self-attention used in decoder blocks of Transformers, especially in generative tasks like text generation.It prevents the model from â€œseeingâ€ future tokens during training â€” ensuring it only attends to the left-side (past and present) tokens.



ğŸ”· 2. Why is Masked Self-Attention needed?
âœ´ï¸ In Generative AI:
When generating a sequence (e.g., text), the model must predict the next word one step at a time, based only on previous words.

If it sees future words during training, itâ€™s like cheating.

ğŸ”’ So, Masking ensures:
âŒ No peeking into future tokens

âœ… True autoregressive behaviour


ğŸ”· 3. How does Masked Self-Attention work?
It modifies the Self-Attention mechanism by adding a mask matrix that blocks (masks) out future positions.

In simple terms:

Normal Self-Attention: All tokens attend to all other tokens

Masked Self-Attention: Token at position I can only attend to tokens at positions â‰¤ I 

ğŸ”· 4. Formula of Masked Self-Attention
| Step                        | Formula                                                   |
| --------------------------- | --------------------------------------------------------- |
| 1. Create Q, K, V           | `Q = XW_Q`, `K = XW_K`, `V = XW_V`                        |
| 2. Compute attention scores | `S = QKáµ€ / âˆšd_k`                                          |
| 3. Apply mask               | `S_masked = S + M`, where **M = -âˆ for future positions** |
| 4. Soft_max + apply to V     | `A = soft_max(S_masked) Ã— V`                               |


ğŸ”· 5. What is the Mask Matrix?
For a sequence of 4 tokens, the mask matrix looks like:
For a sequence of 4 tokens, the mask matrix looks like:

[
 [0, -âˆ, -âˆ, -âˆ],
 [0,  0, -âˆ, -âˆ],
 [0,  0,  0, -âˆ],
 [0,  0,  0,  0]
]



0 means can attend

-âˆ means masked

When passed through soft_max, -âˆ becomes 0 probability.


ğŸ”· 6. Example
Input sequence: "I love ___"
You want the model to predict the next word.

Let tokens be:

Token 1: "I"

Token 2: "love"

Token 3: <mask> (what weâ€™re predicting)

Without masking: Token 1 could attend to Token 2 and 3 â€” cheating!

With masking:

Token 1: attends only to itself

Token 2: attends to 1 and 2

Token 3: attends to 1, 2, 3

But when generating token 3, it should not see its own correct value â€” masking enforces this.


ğŸ”· 7. Where is it used?
| Model              | Masked Self-Attention Used? | Role                              |
| ------------------ | --------------------------- | --------------------------------- |
| GPT (all versions) | âœ… Yes                       | Decoder-only, for generation      |
| BERT               | âŒ No                        | Encoder-only, uses full attention |
| T5 (decoder)       | âœ… Yes                       | Text-to-text generation tasks     |



ğŸ”· 8. Pros and Cons
| Pros âœ…                               | Cons âŒ                                    |
| ------------------------------------ | ----------------------------------------- |
| Prevents information leakage         | Slightly more complex implementation      |
| Enables true autoregressive modelling | Can't use bidirectional context           |
| Essential for generative tasks       | Less accurate in some understanding tasks |


ğŸ”· 9. Summary Table
| Component      | Description                                                                  |
| -------------- | ---------------------------------------------------------------------------- |
| Purpose        | Restrict model from seeing future tokens during training                     |
| Used In        | Decoder part of Transformer (GPT, T5 decoder, etc.)                          |
| Key Operation  | Mask upper triangular part of attention score matrix                         |
| Formula        | `soft_max((QKáµ€ / âˆšd_k) + M) Ã— V`, where M masks future positions              |
| Why Important? | Ensures fairness, enables sequence generation like humans do (left to right) |


Auto regression:- sequential x y z (z works when it should have x , y)
non auto regression:- parallel x y z
encoder:-
decoder:- training prediction 
inference:- prediction


problem with self attention which is solved by masked self attention :- Self-attention sees all tokens (past + future), which causes information leakage during training in generative tasks â€” masked self-attention solves this by blocking future tokens.