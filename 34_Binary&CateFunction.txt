
1. Binary Cross Entropy Loss (Log Loss)
ğŸ“Œ Use Case: Binary classification (2 classes, like spam vs. not spam)
ğŸ“ˆ Formula:
Loss= -[y log(y') + (1-y) log(1-y')]
y âˆˆ is the true label
y' âˆˆ(0,1) is the predicted probability.

âœ… Pros:
Works well when predicting probabilities.

Encourages confidence in correct predictions.

âŒ Cons:
Can lead to exploding gradients if probabilities are too close to 0 or 1.

2. Categorical Cross Entropy Loss
ğŸ“Œ Use Case: Multi-class classification with one-hot encoded labels.
ğŸ“ˆ Formula:
Loss= -âˆ‘Yilog(Y'i)
Yi is  the actual class (1 for the correct class, 0 otherwise),
Y'i is the predicted probability for class i
âœ… Pros:
Excellent for soft max output layers.

Accurately penalizes incorrect class probability.

âŒ Cons:
Not suitable for multi-label problems (use BinaryCrossEntropy instead).

3. Hinge Loss (SVM Loss)
ğŸ“Œ Use Case: Mainly used in Support Vector Machines (SVMs) for binary classification, where labels are -1 or +1.
ğŸ“ˆ Formula:
Loss max(0,1-y.y')
where y' is the raw model op not a probability
y âˆˆ {-1,1} is the true label,

âœ… Pros:
Encourages maximum margin separation.

More robust to small misclassifications.

âŒ Cons:
Doesnâ€™t work with probability outputs.

Only for linear classifiers or margin-based models.

âœ… When to Use Which?
Loss Function	Best For
Binary Cross Entropy	Binary classification with probabilistic outputs
Categorical Cross Entropy	Multi-class classification (softmax output)
Hinge Loss	Binary classification with SVM or margin-based models

In short:

Use Binary Cross Entropy for binary probs (0 or 1).

Use Categorical Cross Entropy for multi-class with softmax.

Use Hinge Loss when using SVM-style training or linear classifiers needing a margin.